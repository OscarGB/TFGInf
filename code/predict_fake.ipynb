{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math, os\n",
    "from sklearn import preprocessing\n",
    "import gmplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Masking, Input, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data():\n",
    "    dfs={}\n",
    "    for i in range(19):\n",
    "        year=str(2000+i)\n",
    "        file='datos/DesmatamentoMunicipios' + year + '.csv'\n",
    "        dfs[i] = pd.read_csv(file, encoding = \"ISO-8859-1\", index_col=0, sep=\",\")\n",
    "        #eliminamos columnas irrelevantes\n",
    "        dfs[i].drop(columns='Latgms', inplace=True)\n",
    "        dfs[i].drop(columns='Longms', inplace=True)\n",
    "        dfs[i].drop(columns='CodIbge', inplace=True)\n",
    "        #df.rename(columns={0:'Latitud', 1:'Longitud', 2:'Municipio', 3:'Estado', 4: 'AreaKm2', 5:'Deforestacion', 6:'Incremento deforestacion', 7:'Bosque', 8:'Nubes', 9:'No observado', 10:'No bosque', 11:'Hidrografia', 12:'Check'} ,inplace=True)\n",
    "        dfs[i].columns=['Latitud', 'Longitud', 'Municipio', 'Estado', 'Area total', 'Deforestacion ' + year, 'Incremento deforestacion ' + year, 'Bosque ' + year, 'Nubes ' + year, 'No observado ' + year, 'No bosque', 'Hidrografia', 'Check ' + year]\n",
    "        dfs[i]=dfs[i][['Latitud', 'Longitud', 'Municipio', 'Estado', 'Area total', 'No bosque', 'Hidrografia', 'Deforestacion ' + year, 'Incremento deforestacion ' + year, 'Bosque ' + year, 'Nubes ' + year, 'No observado ' + year, 'Check ' + year]]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(dfs):\n",
    "    df=dfs[0]\n",
    "    dic={}\n",
    "    for idx, row in df.iterrows():\n",
    "            if row['Municipio'] in dic:\n",
    "                dic[row['Municipio']]+=1\n",
    "            else:\n",
    "                dic[row['Municipio']]=1\n",
    "\n",
    "    repetidos=[]\n",
    "    for key, value in dic.items():\n",
    "        if value==2:\n",
    "            repetidos.append(key)\n",
    "\n",
    "    rep= df.Municipio.isin(repetidos)\n",
    "    mismo_municipio =  df[rep]\n",
    "    print(\"Hay algunos municipios con el mismo nombre en 2 estados distintos:\")\n",
    "    print(mismo_municipio.loc[:,'Latitud':'Area total'])\n",
    "\n",
    "    df=dfs[0]\n",
    "    for idx, value in dfs.items():\n",
    "        value['Municipio']=value['Municipio'] + \" (\" + value['Estado'] + \")\"\n",
    "        if idx > 0:\n",
    "            value.drop(columns='Latitud', inplace=True)\n",
    "            value.drop(columns='Longitud', inplace=True)\n",
    "            value.drop(columns='Estado', inplace=True)\n",
    "            value.drop(columns='Area total', inplace=True)\n",
    "            value.drop(columns='Hidrografia', inplace=True)\n",
    "            value.drop(columns='No bosque', inplace=True)\n",
    "            df=pd.merge(df, value, on='Municipio')\n",
    "\n",
    "    # reorder columns\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[2:4] + cols[0:2] + cols[4:]\n",
    "    df = df[cols]\n",
    "\n",
    "    print(\"Por eso, hacemos merge por la dupla municipio-estado:\")\n",
    "    print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_df(df):\n",
    "    print(\"El dataset viene ordenado por deforestacion, de mayor a menor. Para entrenar el modelo alteramos el orden de forma aleatoria.\")\n",
    "    df = df.sample(frac=1)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(df, name):\n",
    "    print(\"Dibujamos el mapa de \" + str(len(df['Latitud'])) + \" municipios\")\n",
    "    gmap = gmplot.GoogleMapPlotter(df['Latitud'].values[0], df['Longitud'].values[0], 5) # coordenadas del primer municipio del dataframe\n",
    "    gmap.heatmap(df['Latitud'], df['Longitud']) \n",
    "    gmap.draw('mapas/' + name)\n",
    "    print(\"Mapa de calor generado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_fake_data(df, n_replic):\n",
    "    if n_replic < 1:\n",
    "        return\n",
    "    fake_data=[]\n",
    "    std=df.std()\n",
    "    #print(std)\n",
    "    for idx, row in df.iterrows():\n",
    "        for i in range(n_replic):\n",
    "            munic = row['Municipio']+' fake_'+str(i)\n",
    "            estado = row['Estado']\n",
    "            lat = row['Latitud']+std['Latitud']*0.1*np.random.uniform(-1,1) # variacion del 10% de la desviación típica\n",
    "            long = row['Longitud']+std['Longitud']*0.1*np.random.uniform(-1,1) # variacion del 10% de la desviación típica\n",
    "            area = row['Area total']+row['Area total']*0.5*np.random.uniform(-1,1) # variacion del 50% del valor real\n",
    "            no_bosq = row['No bosque']+row['No bosque']*0.5*np.random.uniform(-1,1) # variacion del 10% del valor real\n",
    "            hidro = row['Hidrografia']+row['Hidrografia']*0.5*np.random.uniform(-1,1) # variacion del 10% del valor real\n",
    "            temp = []\n",
    "            for j in range(19):    #TODO: Normalizar por año y append todas a la vez, no se puede cambiar el area cada año\n",
    "                #year = 2000 + j\n",
    "                if len(temp) > 5:\n",
    "                    incr = row[n_vars_temp*j+8]   # 7 vars fijas + 1 defor para llegar al incremento\n",
    "                    proporcion = incr/row['Area total']\n",
    "                    t = area*proporcion\n",
    "                    incr = t+t*0.05*np.random.uniform(-1,1)\n",
    "                    defor = temp[-6] + incr\n",
    "                    bosque = temp[-4] - incr\n",
    "                    temp.extend([defor,incr, bosque])\n",
    "                else:\n",
    "                    proporcion = row[j+7]/row['Area total']\n",
    "                    t = area*proporcion\n",
    "                    defor = t+t*0.05*np.random.uniform(0,1)\n",
    "                    bosque = area - (defor + no_bosq + hidro)\n",
    "                    temp.extend([defor,float('NaN'), bosque])\n",
    "                nubes = 0\n",
    "                no_obs = 0\n",
    "                suma = defor + no_bosq + hidro + bosque + nubes + no_obs\n",
    "                check = suma/area*100\n",
    "                temp.extend([nubes, no_obs, check])\n",
    "            fixed = [munic, estado, lat, long, area, no_bosq, hidro]     \n",
    "            fake_row = np.concatenate((fixed, temp))\n",
    "            fake_data.append(fake_row)\n",
    "    fake_df = pd.DataFrame(fake_data, columns=df.columns)\n",
    "    fake_df = fake_df.infer_objects()\n",
    "    return fake_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(temporal_vars, year_ini_train=0, year_ini_test=1, window_size=18, n_vars_temp=6):\n",
    "    # ensure all data is float\n",
    "    temporal_vars = temporal_vars.astype('float32')\n",
    "    temporal_train = temporal_vars.iloc[:, n_vars_temp*year_ini_train:n_vars_temp*(year_ini_train+window_size)]\n",
    "    temporal_test = temporal_vars.iloc[0:760, n_vars_temp*year_ini_test:] #testeamos con municipios reales\n",
    "    return temporal_train, temporal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(temporal_train, fixed_vars, n_vars_temp):\n",
    "    # data structure for LSTM\n",
    "    # normalize features\n",
    "    x = temporal_train.iloc[:,0:-n_vars_temp].values\n",
    "    y = temporal_train.iloc[:,-n_vars_temp].values.reshape(-1, 1) # reshape in 2D, we get 'deforestacion' as y\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x)\n",
    "    scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "    y = scaler2.fit_transform(y)\n",
    "    #print(temporal_train.shape, x.shape, y.shape)\n",
    "    # structure in arrays to be the input of the LSTM\n",
    "    vars_lstm=[]\n",
    "    municipio_len=len(x[0])\n",
    "    for mun in x:\n",
    "        municipio=[]\n",
    "        j=0\n",
    "        while j < municipio_len:\n",
    "            m=mun[j:j+n_vars_temp]\n",
    "            if j==0:\n",
    "                m[1]=-1\t\t# marking missing values: Incremento deforestacion 1999/2000\n",
    "            #elif m[5]!=municipio[0][5] or m[6]!=municipio[0][6]: # Hidrografia y No bosque son variables fijas!!!\n",
    "            #\tprint(\"Hidrografia y No bosque varian de año en año\")\n",
    "            municipio.append(mun[j:j+n_vars_temp])\n",
    "            j+=n_vars_temp\n",
    "        vars_lstm.append(municipio)\n",
    "\n",
    "    vars_lstm=np.array(vars_lstm)\n",
    "    X1 = vars_lstm\n",
    "\n",
    "    # data structure for Dense\n",
    "    # One hot encoding \n",
    "    fixed_vars = pd.concat([fixed_vars,pd.get_dummies(fixed_vars['Estado'])],axis=1)\n",
    "    # Drop column as it is now encoded\n",
    "    fixed_vars = fixed_vars.drop('Estado',axis = 1)\n",
    "    scaler3 = MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler3.fit_transform(fixed_vars.iloc[:,1:]) # nombre del municipio en la primera columna\n",
    "    X2 = np.array(x)\n",
    "    return X1, X2, y, scaler, scaler2, scaler3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(window_size, n_vars_temp, hidden_layers, mask=1):\n",
    "    # LSTM para variables temporales\n",
    "    input_temporal = Input(shape=(window_size-1, n_vars_temp))\n",
    "    if mask==0:\n",
    "        lstm = LSTM(5, activation='relu')(input_temporal) #, unroll =True) -> check what is is\n",
    "    else:\n",
    "        masking = Masking(mask_value=-1)(input_temporal)\n",
    "        lstm = LSTM(5, activation='relu', dropout=0.2)(masking)\n",
    "        #lstm_1 = LSTM(5, activation='relu', return_sequences = True)(masking)\n",
    "        #lstm_2 = LSTM(5, activation='relu')(lstm_1)\n",
    "    dense_1 = Dense(16, activation = 'relu')(lstm)\n",
    "    \n",
    "    # Dense para variables fijas\n",
    "    input_fijo = Input(shape=(X2.shape[1],))\n",
    "    dense_2 = Dense(32, activation = 'relu')(input_fijo)\n",
    "    \n",
    "    # concateno las 2 redes\n",
    "    merge = concatenate([dense_1, dense_2])\n",
    "    dense = [merge]\n",
    "    for i in range(hidden_layers):\n",
    "        dense.append(Dense(16, activation = 'relu')(dense[-1]))\n",
    "    output = Dense(1, activation = 'relu')(dense[-1])\n",
    "    model = Model(inputs=[input_temporal, input_fijo], outputs=output)\n",
    "    \n",
    "    # summarize layers\n",
    "    print(model.summary())\n",
    "    # plot graph\n",
    "    plot_model(model, to_file='modelos/model_'+str(hidden_layers)+'.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(temporal_test, n_vars_temp, scaler, scaler2, scaler3):\n",
    "    # data structure for LSTM\n",
    "    # normalize features\n",
    "    x = temporal_test.values[:,0:-n_vars_temp]\n",
    "    y = temporal_test.values[:,-n_vars_temp].reshape(-1, 1) # reshape in 2D\n",
    "    x = scaler.fit_transform(x)\n",
    "    y = scaler2.fit_transform(y)\n",
    "    # structure in arrays to be the input of the LSTM\n",
    "    vars_lstm=[]\n",
    "    municipio_len=len(x[0])\n",
    "    for mun in x:\n",
    "        municipio=[]\n",
    "        j=0\n",
    "        while j < municipio_len:\n",
    "            m=mun[j:j+n_vars_temp]\n",
    "            if j==0:\n",
    "                m[1]=-1\t\t# marking missing values: Incremento deforestacion 1999/2000\n",
    "            #elif m[5]!=municipio[0][5] or m[6]!=municipio[0][6]: # Hidrografia y No bosque son variables fijas!!!\n",
    "            #\tprint(\"Hidrografia y No bosque varian de año en año\")\n",
    "            municipio.append(mun[j:j+n_vars_temp])\n",
    "            j+=n_vars_temp\n",
    "        vars_lstm.append(municipio)\n",
    "\n",
    "    vars_lstm=np.array(vars_lstm)\n",
    "    X1 = vars_lstm\n",
    "    return X1, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(municipios_reales, y, y_pred):\n",
    "    # rescaling\n",
    "    y=scaler2.inverse_transform(y).flatten()\n",
    "    y_pred=scaler2.inverse_transform(y_pred).flatten()\n",
    "    \n",
    "    errores = []\n",
    "    abajo = 0\n",
    "    num_test = len(y_pred)\n",
    "    for i in range(num_test):\n",
    "        delta = y[i] - y_pred[i]\n",
    "        if delta > 0:\n",
    "            abajo += 1\n",
    "        error = abs(delta)\n",
    "        #print(municipios_reales[i], 'Expected', y[i], 'Predicted', y_pred[i], 'Error', str(error))\n",
    "        errores.append(error)\n",
    "    test = {'Municipio': municipios_reales.values, 'Expected': y[0:760], 'Predicted': y_pred, 'Error': errores}\n",
    "    test = pd.DataFrame(test)\n",
    "    print('Error absoluto medio', np.array(errores).mean(), \"con una muestra de\", num_test, \"municipios\")\n",
    "    print('La predicción es menor que el dato real',abajo, \"veces\")\n",
    "    print(test)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = obtain_data()\n",
    "df = create_df(dfs)\n",
    "\n",
    "print(\"\\nEl area total cubierta por los municipios es: \" + str(df.sum(axis = 0, skipna = True)[4]) + \" km^2, mientras el area total de la Amazonia es de 5,5M km^2.\")\n",
    "print(\"Por ello, podemos afirmar que estamos cubriendo una superficie que supone el 92% del total.\\n\")\n",
    "\n",
    "plot_map(df, 'mapa.html')\n",
    "\n",
    "n_vars_temp = df.loc[:,'Deforestacion 2000':].shape[1]//19 # number of variables per year\n",
    "\n",
    "#shuffle data\n",
    "df = shuffle_df(df)\n",
    "\n",
    "results = []\n",
    "n_replics = [0,1,5,10,50,100]\n",
    "for n in n_replics:\n",
    "    if n == 0:\n",
    "        df_complete = df\n",
    "    else:\n",
    "        fake_df = generate_fake_data(df, n)\n",
    "        df_complete = df.append(fake_df)\n",
    "    #df_complete = df_complete.sort_values(by=['Municipio']) # to be ommited for the test: 760 primeros municipios son los reales\n",
    "    df_complete.loc[:,'Latitud':] = df_complete.loc[:,'Latitud':].astype('float')\n",
    "    df_complete = df_complete.infer_objects()\n",
    "    \n",
    "    #plot map\n",
    "    name = 'mapa_fake' + str(n) + '.html'\n",
    "    plot_map(df_complete, name)\n",
    "\n",
    "    #shuffle data\n",
    "    df_complete[760:] = shuffle_df(df_complete[760:])\n",
    "    \n",
    "    # structure data\n",
    "    fixed_vars = df_complete.loc[:,['Municipio', 'Estado', 'Latitud', 'Longitud', 'Area total', 'Hidrografia', 'No bosque']]\n",
    "    temporal_vars = df_complete.loc[:,'Deforestacion 2000':]\n",
    "    window_size=18\n",
    "    temporal_train, temporal_test = series_to_supervised(temporal_vars, 0, 1, window_size, n_vars_temp)\n",
    "    \n",
    "    # preprocess data for training\n",
    "    X1, X2, y, scaler, scaler2, scaler3 = preprocess_train(temporal_train, fixed_vars, n_vars_temp)\n",
    "    \n",
    "    # create model\n",
    "    hidden_layers=[1,2,3]\n",
    "    epochs=[5,10,20,50]\n",
    "    batch_sizes=[32,64]\n",
    "    for h in hidden_layers:\n",
    "        for e in epochs:\n",
    "            for bs in batch_sizes:\n",
    "                print(\"Creando el modelo...\")\n",
    "                print(\"\\tNúmero de épocas:\", e)\n",
    "                print('\\tBatch size:', bs)\n",
    "                print('\\tCapas intermedias', h)\n",
    "                print('\\tMuestra para training:', len(df_complete), 'municipios')\n",
    "                model = create_model(window_size, n_vars_temp, h)\n",
    "                model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mae'])\n",
    "                # fit model\n",
    "                train_size = int(len(X1)*0.8)\n",
    "                checkpoint_cb = ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True, verbose=1)\n",
    "                model.fit([X1[0:train_size], X2[0:train_size]], y[0:train_size], epochs=e, batch_size=32, verbose=2, validation_data=([X1[train_size:], X2[train_size:]], y[train_size:]), callbacks=[checkpoint_cb])\n",
    "\n",
    "                # preprocess data for testing\n",
    "                X1_test, y_test = preprocess_test(temporal_test, n_vars_temp, scaler, scaler2, scaler3)\n",
    "                X2_test = X2[0:760]\n",
    "                municipios_reales = fixed_vars.iloc[0:760,0]\n",
    "\n",
    "                # evaluate model on new data\n",
    "                model = load_model(\"my_keras_model.h5\")\n",
    "                y_pred = model.predict([X1_test, X2_test])    \n",
    "                test = evaluate_test(municipios_reales, y, y_pred)\n",
    "\n",
    "                d = {}\n",
    "                d['Municipios'] = len(df_complete)\n",
    "                d['Epocas'] = e\n",
    "                d['Batch size'] = bs\n",
    "                d['Capas intermedias'] = h\n",
    "                d['Error absoluto medio'] = test['Error'].mean()\n",
    "                results.append(d)\n",
    "\n",
    "# Check if data folder already created. If not, create it\n",
    "if not os.path.exists('resultados/'):\n",
    "    os.makedirs('resultados/')\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('resultados/precision_configuraciones_modelo.csv') # results of the different configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('resultados/precision_configuraciones_modelo.csv') # results of the different configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
